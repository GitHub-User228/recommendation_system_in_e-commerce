import warnings

warnings.filterwarnings("ignore")

import mlflow
import pandas as pd
from pathlib import Path
from abc import ABC, abstractmethod
from mlflow.tracking import MlflowClient

from scripts import logger
from scripts.env import env_vars
from scripts.evaluate import evaluate_recommendations
from scripts.utils import save_yaml, read_yaml, get_experiment_id


class BaseComponent(ABC):
    """
    The `BaseComponent` class is an abstract base class that
    defines the common functionality for all components in
    the system.
    """

    def log(self) -> None:
        """
        Logs the current run via MLFlow
        """

        # Get the host
        host = "localhost"
        if self.is_airflow:
            host = env_vars.mlflow_server_host

        # Setup the MLFlow client
        mlflow.set_tracking_uri(f"http://{host}:{env_vars.mlflow_server_port}")
        mlflow.set_registry_uri(f"http://{host}:{env_vars.mlflow_server_port}")
        client = MlflowClient()

        run_name = self.config.run_name
        cat = "not_testing"
        if self.is_testing:
            run_name = f"{run_name}__testing"
            cat = "testing"

        # Get the experiment id
        experiment_id = get_experiment_id(
            experiment_name=self.config.experiment_name, client=client
        )

        # Log the run
        with mlflow.start_run(
            run_name=run_name, experiment_id=experiment_id
        ) as run:

            path = self.config.destination_path
            if not isinstance(path, Path):
                path = path[cat]

            # Logging the data
            mlflow.log_artifacts(local_dir=path)

            # Logging the current script
            mlflow.log_artifact(
                local_path=self._path_to_script,
                artifact_path="scripts",
            )

            # Logging settings script
            mlflow.log_artifact(
                local_path=Path(Path(__file__).parent.parent, "settings.py"),
                artifact_path="scripts",
            )

            # Logging config files
            mlflow.log_artifact(
                local_path=Path(env_vars.config_dir, "components.yaml"),
                artifact_path="config",
            )
            mlflow.log_artifact(
                local_path=Path(env_vars.config_dir, "dates.yaml"),
                artifact_path="config",
            )

            # Logging metrics (if they exist)
            if hasattr(self.config, "metrics_filename"):
                if Path(path, self.config.metrics_filename).exists():
                    mlflow.log_metrics(
                        read_yaml(Path(path, self.config.metrics_filename))
                    )


class BaseModelComponent(BaseComponent):
    """
    The `BaseModelComponent` class is an abstract base class that
    defines the common functionality for all model components in
    the system.
    """

    @abstractmethod
    def fit(self) -> None:
        """
        Fit the model to the data.
        This method must be implemented by subclasses.
        """
        pass

    @abstractmethod
    def recommend(self):
        """
        Generate recommendations.
        This method must be implemented by subclasses.
        """
        pass

    def evaluate(self) -> None:
        """
        Evaluates the recommendations generated by the model on the
        test set.
        """

        if not self.is_testing:
            msg = "Unable to evaluate when the model is not in testing mode."
            logger.error(msg)
            raise ValueError(msg)

        # Reading data with recommendations
        df = pd.read_parquet(
            Path(
                self.config.destination_path["testing"],
                self.config.recommendations_filenames["test"],
            )
        )

        # Reading data with real interactions
        df_real = (
            pd.read_parquet(
                Path(
                    self.config.source_path,
                    self.config.events_filenames["test"],
                )
            )
            .query(f"rating >= 2")[
                [self.config.fields_id["user"], self.config.fields_id["item"]]
            ]
            .drop_duplicates()
        )

        # Evaluating recommendations
        metrics = evaluate_recommendations(
            user_items_real=df_real,
            user_items_pred=df,
            user_id_col=self.config.fields_id["user"],
            item_id_col=self.config.fields_id["item"],
        )

        # Saving metrics
        save_yaml(
            path=Path(
                self.config.destination_path["testing"],
                self.config.metrics_filename,
            ),
            data=metrics,
        )
